{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee13ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.187.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-python-client) (0.31.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-python-client) (2.43.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-python-client) (0.2.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (6.33.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9734e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.3.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253efaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a639a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from seaborn) (2.2.5)\n",
      "Requirement already satisfied: pandas>=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45aa975",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/henryfuckingdopeman/Desktop/DSCI-510/../data/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved cleaned content ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcon_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     df_art_raw, df_con_raw = \u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df_art_raw.empty \u001b[38;5;129;01mor\u001b[39;00m df_con_raw.empty:\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo raw data to clean.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_raw_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_raw_data\u001b[39m():\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Load the most recent raw JSON files.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    Your get_data.py saves files as:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m    So we match that pattern.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m     artist_files = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f.startswith(\u001b[33m\"\u001b[39m\u001b[33martist_summary_\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m     26\u001b[39m         reverse=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m     )\n\u001b[32m     28\u001b[39m     content_files = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m     29\u001b[39m         [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(RAW_DATA_DIR) \u001b[38;5;28;01mif\u001b[39;00m f.startswith(\u001b[33m\"\u001b[39m\u001b[33mcontent_detail_\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m     30\u001b[39m         reverse=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m artist_files \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content_files:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/henryfuckingdopeman/Desktop/DSCI-510/../data/raw'"
     ]
    }
   ],
   "source": [
    "# src/clean_data.py\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Safe directory handling (works in VS Code + Jupyter) ---\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, '../data/raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, '../data/processed')\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"\n",
    "    Load the most recent raw JSON files.\n",
    "    Your get_data.py saves files as:\n",
    "        artist_summary_YYYYMMDD.json\n",
    "        content_detail_YYYYMMDD.json\n",
    "    So we match that pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    artist_files = sorted(\n",
    "        [f for f in os.listdir(RAW_DATA_DIR) if f.startswith(\"artist_summary_\")],\n",
    "        reverse=True\n",
    "    )\n",
    "    content_files = sorted(\n",
    "        [f for f in os.listdir(RAW_DATA_DIR) if f.startswith(\"content_detail_\")],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    if not artist_files or not content_files:\n",
    "        print(\"No raw JSON files found. Run get_data.py first.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Load most recent pair\n",
    "    artist_path = os.path.join(RAW_DATA_DIR, artist_files[0])\n",
    "    content_path = os.path.join(RAW_DATA_DIR, content_files[0])\n",
    "\n",
    "    print(f\"Loading artist summary ‚Üí {artist_path}\")\n",
    "    with open(artist_path, \"r\") as f:\n",
    "        artists_raw = json.load(f)\n",
    "\n",
    "    print(f\"Loading content detail ‚Üí {content_path}\")\n",
    "    with open(content_path, \"r\") as f:\n",
    "        content_raw = json.load(f)\n",
    "\n",
    "    df_artists = pd.DataFrame(artists_raw)\n",
    "    df_content = pd.DataFrame(content_raw)\n",
    "\n",
    "    return df_artists, df_content\n",
    "\n",
    "\n",
    "def clean_and_process_data(df_art_raw, df_content_raw):\n",
    "    \"\"\"\n",
    "    CLEANING RULES FOR YOUR PROJECT:\n",
    "    - No Spotify data (removed)\n",
    "    - Artists have:\n",
    "        tiktok_views, tiktok_video_count,\n",
    "        subscriber_count, view_count, video_count\n",
    "    - Content has YouTube videos with:\n",
    "        view_count, like_count, comment_count,\n",
    "        sentiment ratios, published_at\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Cleaning data...\")\n",
    "\n",
    "    # --- Clean artist-level summary ---\n",
    "    df_art = df_art_raw.copy()\n",
    "\n",
    "    # Convert numeric cols if exist\n",
    "    for col in [\"tiktok_views\", \"tiktok_video_count\",\n",
    "                \"subscriber_count\", \"view_count\", \"video_count\"]:\n",
    "        if col in df_art.columns:\n",
    "            df_art[col] = pd.to_numeric(df_art[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # timestamp formatting\n",
    "    if \"timestamp\" in df_art.columns:\n",
    "        df_art[\"timestamp\"] = pd.to_datetime(df_art[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop garbage rows (shouldn't happen)\n",
    "    if \"artist_name\" in df_art.columns:\n",
    "        df_art.dropna(subset=[\"artist_name\"], inplace=True)\n",
    "\n",
    "    # --- Clean content-level data (YouTube videos) ---\n",
    "    df_con = df_content_raw.copy()\n",
    "\n",
    "    # Parse timestamps\n",
    "    if \"timestamp\" in df_con.columns:\n",
    "        df_con[\"timestamp\"] = pd.to_datetime(df_con[\"timestamp\"], errors=\"coerce\")\n",
    "    if \"published_at\" in df_con.columns:\n",
    "        df_con[\"published_at\"] = pd.to_datetime(df_con[\"published_at\"], errors=\"coerce\")\n",
    "\n",
    "    # Make numeric\n",
    "    for col in [\"view_count\", \"like_count\", \"comment_count\"]:\n",
    "        if col in df_con.columns:\n",
    "            df_con[col] = pd.to_numeric(df_con[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Engagement rate\n",
    "    if all(c in df_con.columns for c in [\"like_count\", \"comment_count\", \"view_count\"]):\n",
    "        df_con[\"engagement_rate\"] = (\n",
    "            (df_con[\"like_count\"] + df_con[\"comment_count\"]) /\n",
    "            df_con[\"view_count\"].replace(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        df_con[\"engagement_rate\"] = 0\n",
    "\n",
    "    # Sentiment fallback\n",
    "    for col in [\"sentiment_positive_ratio\", \"sentiment_negative_ratio\", \"sentiment_neutral_ratio\"]:\n",
    "        if col in df_con.columns:\n",
    "            df_con[col] = df_con[col].fillna(0)\n",
    "        else:\n",
    "            df_con[col] = 0\n",
    "\n",
    "    # Clean artist name\n",
    "    if \"artist_name\" in df_con.columns:\n",
    "        df_con.dropna(subset=[\"artist_name\"], inplace=True)\n",
    "\n",
    "    print(\"Cleaning complete.\")\n",
    "    return df_art, df_con\n",
    "\n",
    "\n",
    "def save_processed_data(df_art, df_con):\n",
    "    art_path = os.path.join(PROCESSED_DATA_DIR, \"cleaned_artists_data.csv\")\n",
    "    con_path = os.path.join(PROCESSED_DATA_DIR, \"cleaned_content_data.csv\")\n",
    "\n",
    "    df_art.to_csv(art_path, index=False)\n",
    "    df_con.to_csv(con_path, index=False)\n",
    "\n",
    "    print(f\"Saved cleaned artists ‚Üí {art_path}\")\n",
    "    print(f\"Saved cleaned content ‚Üí {con_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_art_raw, df_con_raw = load_raw_data()\n",
    "    if df_art_raw.empty or df_con_raw.empty:\n",
    "        print(\"No raw data to clean.\")\n",
    "    else:\n",
    "        df_art, df_con = clean_and_process_data(df_art_raw, df_con_raw)\n",
    "        save_processed_data(df_art, df_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube API key missing ‚Äî skipping YouTube collection.\n",
      "\n",
      "Collecting data for Taylor Swift...\n",
      "\n",
      "Collecting data for NBA YoungBoy...\n",
      "\n",
      "Collecting data for Adele...\n",
      "\n",
      "Collecting data for Bad Bunny...\n",
      "\n",
      "Collecting data for Billie Eilish...\n",
      "\n",
      "Collecting data for Drake...\n",
      "\n",
      "Collecting data for The Weeknd...\n",
      "\n",
      "Collecting data for Doja Cat...\n",
      "\n",
      "Collecting data for Post Malone...\n",
      "\n",
      "Collecting data for Kendrick Lamar...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/../data/raw/artist_summary_20251210_152657.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 243\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    242\u001b[39m     now = datetime.now()\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[43mcollect_and_save_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARTISTS_TO_ANALYZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mcollect_and_save_data\u001b[39m\u001b[34m(artists_list, timestamp)\u001b[39m\n\u001b[32m    225\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    227\u001b[39m ts = timestamp.strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martist_summary_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mts\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    229\u001b[39m     json.dump(summary, f, indent=\u001b[32m4\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(RAW_DATA_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcontent_detail_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/../data/raw/artist_summary_20251210_152657.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- YouTube API ---\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "# Output dir\n",
    "#RAW_DATA_DIR = os.path.join(os.path.dirname(__file__), '../data/raw')\n",
    "#os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, '../data/raw')\n",
    "\n",
    "# Initialize YouTube\n",
    "youtube = None\n",
    "if YOUTUBE_API_KEY:\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "        print(\"YouTube API client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(\"YouTube init error:\", e)\n",
    "else:\n",
    "    print(\"YouTube API key missing ‚Äî skipping YouTube collection.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "#  üî• TikTok Scraping\n",
    "# -------------------------------------------\n",
    "def scrape_tiktok_hashtag_stats(hashtag):\n",
    "    url = f\"https://www.tiktok.com/tag/{hashtag}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for script in soup.find_all(\"script\"):\n",
    "            if \"props\" in script.text:\n",
    "                txt = script.text\n",
    "\n",
    "                if '\"viewCount\":' in txt:\n",
    "                    v1 = txt.index('\"viewCount\":') + len('\"viewCount\":')\n",
    "                    v2 = txt.index(\",\", v1)\n",
    "                    views = int(txt[v1:v2].strip())\n",
    "\n",
    "                    if '\"videoCount\":' in txt:\n",
    "                        c1 = txt.index('\"videoCount\":') + len('\"videoCount\":')\n",
    "                        c2 = txt.index(\",\", c1)\n",
    "                        vids = int(txt[c1:c2].strip())\n",
    "\n",
    "                        return {\n",
    "                            \"hashtag\": hashtag,\n",
    "                            \"tiktok_views\": views,\n",
    "                            \"tiktok_video_count\": vids\n",
    "                        }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"TikTok scrape failed for #{hashtag}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "#  üî• YouTube Stats\n",
    "# -------------------------------------------\n",
    "def get_youtube_channel_stats(channel_id):\n",
    "    if not youtube:\n",
    "        return None\n",
    "    try:\n",
    "        req = youtube.channels().list(part=\"statistics,snippet\", id=channel_id)\n",
    "        res = req.execute()\n",
    "        if not res[\"items\"]:\n",
    "            return None\n",
    "\n",
    "        stats = res[\"items\"][0][\"statistics\"]\n",
    "        snip = res[\"items\"][0][\"snippet\"]\n",
    "\n",
    "        return {\n",
    "            \"channel_id\": channel_id,\n",
    "            \"channel_name\": snip[\"title\"],\n",
    "            \"subscriber_count\": int(stats.get(\"subscriberCount\", 0)),\n",
    "            \"view_count\": int(stats.get(\"viewCount\", 0)),\n",
    "            \"video_count\": int(stats.get(\"videoCount\", 0)),\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_youtube_video_stats_and_comments(channel_id, max_videos=10, max_comments=10):\n",
    "    if not youtube:\n",
    "        return []\n",
    "\n",
    "    videos = []\n",
    "    try:\n",
    "        search = youtube.search().list(\n",
    "            part=\"id,snippet\", channelId=channel_id,\n",
    "            maxResults=max_videos, order=\"date\", type=\"video\"\n",
    "        )\n",
    "        res = search.execute()\n",
    "        ids = [i[\"id\"][\"videoId\"] for i in res[\"items\"]]\n",
    "\n",
    "        if not ids:\n",
    "            return []\n",
    "\n",
    "        stats_req = youtube.videos().list(part=\"statistics,snippet\", id=\",\".join(ids))\n",
    "        stats_res = stats_req.execute()\n",
    "\n",
    "        for item in stats_res[\"items\"]:\n",
    "            vid = {\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"video_title\": item[\"snippet\"][\"title\"],\n",
    "                \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "            }\n",
    "\n",
    "            # Sentiment\n",
    "            comments = get_youtube_comments_for_sentiment(item[\"id\"], max_comments)\n",
    "            pos = neg = neu = 0\n",
    "\n",
    "            for c in comments:\n",
    "                p = TextBlob(c).sentiment.polarity\n",
    "                if p > 0.1: pos += 1\n",
    "                elif p < -0.1: neg += 1\n",
    "                else: neu += 1\n",
    "\n",
    "            total = len(comments) or 1\n",
    "            vid[\"sentiment_positive_ratio\"] = pos / total\n",
    "            vid[\"sentiment_negative_ratio\"] = neg / total\n",
    "            vid[\"sentiment_neutral_ratio\"] = neu / total\n",
    "\n",
    "            videos.append(vid)\n",
    "        return videos\n",
    "\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_youtube_comments_for_sentiment(video_id, max_comments=10):\n",
    "    comments = []\n",
    "    try:\n",
    "        req = youtube.commentThreads().list(\n",
    "            part=\"snippet\", videoId=video_id,\n",
    "            maxResults=max_comments, textFormat=\"plainText\"\n",
    "        )\n",
    "        res = req.execute()\n",
    "\n",
    "        for item in res.get(\"items\", []):\n",
    "            comments.append(\n",
    "                item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# üé§ Artist List (10 artists)\n",
    "# -------------------------------------------\n",
    "ARTISTS_TO_ANALYZE = [\n",
    "    {\"name\": \"Taylor Swift\", \"tiktok_tag\": \"taylorswift\", \"youtube_channel_id\": \"UCqfmriSjJ_k4C8W6J_k7J_g\"},\n",
    "    {\"name\": \"NBA YoungBoy\", \"tiktok_tag\": \"nbayoungboy\", \"youtube_channel_id\": \"UCNofc_JcK-0FfdJ_YkE6VBg\"},\n",
    "    {\"name\": \"Adele\", \"tiktok_tag\": \"adele\", \"youtube_channel_id\": \"UCRw-9o3C02JkL4o1CjDkywA\"},\n",
    "    {\"name\": \"Bad Bunny\", \"tiktok_tag\": \"badbunny\", \"youtube_channel_id\": \"UCgCHiixL-q7L5_Fv2EaV3-w\"},\n",
    "    {\"name\": \"Billie Eilish\", \"tiktok_tag\": \"billieeilish\", \"youtube_channel_id\": \"UCiGm_E4ZwYVaeYBjfK6edYA\"},\n",
    "    {\"name\": \"Drake\", \"tiktok_tag\": \"drake\", \"youtube_channel_id\": \"UCByOQJjav0CUDwxCk-jVNRQ\"},\n",
    "    {\"name\": \"The Weeknd\", \"tiktok_tag\": \"theweeknd\", \"youtube_channel_id\": \"UC0WP5P-ufpRfjbNrmOWwLBQ\"},\n",
    "    {\"name\": \"Doja Cat\", \"tiktok_tag\": \"dojacat\", \"youtube_channel_id\": \"UCzvK5p4gGg9Q2HfKF3Ab4Qw\"},\n",
    "    {\"name\": \"Post Malone\", \"tiktok_tag\": \"postmalone\", \"youtube_channel_id\": \"UC3gK4uQkzkG4gQ2vRGLcH3A\"},\n",
    "    {\"name\": \"Kendrick Lamar\", \"tiktok_tag\": \"kendricklamar\", \"youtube_channel_id\": \"UC3lBXkFNkSgWunf6Z64MfKQ\"}\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# üî• Main collection\n",
    "# -------------------------------------------\n",
    "def collect_and_save_data(artists_list, timestamp):\n",
    "    summary = []\n",
    "    detail = []\n",
    "\n",
    "    for a in artists_list:\n",
    "        print(f\"\\nCollecting data for {a['name']}...\")\n",
    "\n",
    "        # TikTok\n",
    "        tiktok = scrape_tiktok_hashtag_stats(a[\"tiktok_tag\"])\n",
    "        if tiktok:\n",
    "            tiktok.update({\n",
    "                \"artist_name\": a[\"name\"],\n",
    "                \"data_type\": \"tiktok_stats\",\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "            })\n",
    "            summary.append(tiktok)\n",
    "\n",
    "        # YouTube channel\n",
    "        yt = get_youtube_channel_stats(a[\"youtube_channel_id\"])\n",
    "        if yt:\n",
    "            yt.update({\n",
    "                \"artist_name\": a[\"name\"],\n",
    "                \"data_type\": \"youtube_channel_stats\",\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "            })\n",
    "            summary.append(yt)\n",
    "\n",
    "        # YouTube videos\n",
    "        vids = get_youtube_video_stats_and_comments(a[\"youtube_channel_id\"])\n",
    "        for v in vids:\n",
    "            v.update({\n",
    "                \"artist_name\": a[\"name\"],\n",
    "                \"data_type\": \"youtube_video\",\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "            })\n",
    "            detail.append(v)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    ts = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(os.path.join(RAW_DATA_DIR, f\"artist_summary_{ts}.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(RAW_DATA_DIR, f\"content_detail_{ts}.json\"), \"w\") as f:\n",
    "        json.dump(detail, f, indent=4)\n",
    "\n",
    "    print(\"\\n‚úî Data saved successfully.\")\n",
    "    return summary, detail\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Run script\n",
    "# -------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    now = datetime.now()\n",
    "    collect_and_save_data(ARTISTS_TO_ANALYZE, now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8840ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- Directory handling that works in VS Code + Jupyter ---\u001b[39;00m\n\u001b[32m      8\u001b[39m BASE_DIR = os.path.dirname(os.path.abspath(\u001b[34m__file__\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m os.getcwd()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# src/run_analysis.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Directory handling that works in VS Code + Jupyter ---\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, '../data/processed')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, '../results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "CLEANED_ARTISTS_CSV = os.path.join(PROCESSED_DATA_DIR, 'cleaned_artists_data.csv')\n",
    "CLEANED_CONTENT_CSV = os.path.join(PROCESSED_DATA_DIR, 'cleaned_content_data.csv')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Helper utilities\n",
    "# ----------------------\n",
    "def first_existing_column(df, candidates, default=None):\n",
    "    \"\"\"Return first column name in candidates that exists in df.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return default\n",
    "\n",
    "\n",
    "def safe_to_datetime(series):\n",
    "    \"\"\"Convert to datetime without errors.\"\"\"\n",
    "    return pd.to_datetime(series, errors='coerce')\n",
    "\n",
    "\n",
    "def manual_minmax(series):\n",
    "    \"\"\"Min-max normalize using Python math instead of sklearn.\"\"\"\n",
    "    series = pd.to_numeric(series, errors='coerce').fillna(0)\n",
    "    mn, mx = series.min(), series.max()\n",
    "    if mx - mn == 0:\n",
    "        return pd.Series([0] * len(series))\n",
    "    return (series - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Load cleaned data\n",
    "# ----------------------\n",
    "def load_cleaned_data():\n",
    "    if not os.path.exists(CLEANED_ARTISTS_CSV) or not os.path.exists(CLEANED_CONTENT_CSV):\n",
    "        print(\"‚ùå Cleaned data missing. Run clean_data.py first.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    df_artists = pd.read_csv(CLEANED_ARTISTS_CSV)\n",
    "    df_content = pd.read_csv(CLEANED_CONTENT_CSV)\n",
    "\n",
    "    # Fix timestamps\n",
    "    for col in ['collection_timestamp', 'published_at', 'release_date']:\n",
    "        if col in df_artists.columns:\n",
    "            df_artists[col] = safe_to_datetime(df_artists[col])\n",
    "        if col in df_content.columns:\n",
    "            df_content[col] = safe_to_datetime(df_content[col])\n",
    "\n",
    "    return df_artists, df_content\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Main Analysis\n",
    "# ----------------------\n",
    "def perform_analysis(df_artists, df_content):\n",
    "\n",
    "    print(\"‚öôÔ∏è Running analysis...\")\n",
    "\n",
    "    # ---------- YOUTUBE ENGAGEMENT ----------\n",
    "    view_col = first_existing_column(df_content, ['view_count', 'yt_view_count'])\n",
    "    like_col = first_existing_column(df_content, ['like_count'])\n",
    "    comment_col = first_existing_column(df_content, ['comment_count'])\n",
    "\n",
    "    df_content['views'] = pd.to_numeric(df_content[view_col], errors='coerce').fillna(0)\n",
    "    df_content['likes'] = pd.to_numeric(df_content[like_col], errors='coerce').fillna(0)\n",
    "    df_content['comments'] = pd.to_numeric(df_content[comment_col], errors='coerce').fillna(0)\n",
    "\n",
    "    df_content['engagement_rate'] = df_content.apply(\n",
    "        lambda r: (r['likes'] + r['comments']) / r['views'] if r['views'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Avg engagement per artist\n",
    "    avg_eng = df_content.groupby('artist_name')['engagement_rate'].mean().reset_index()\n",
    "    avg_eng.rename(columns={'engagement_rate': 'avg_yt_engagement'}, inplace=True)\n",
    "\n",
    "    df_artists = df_artists.merge(avg_eng, on='artist_name', how='left')\n",
    "    df_artists['avg_yt_engagement'] = df_artists['avg_yt_engagement'].fillna(0)\n",
    "\n",
    "\n",
    "    # ---------- NUMERIC FIX for artist-level stats ----------\n",
    "    yt_sub_col = first_existing_column(df_artists, ['yt_subscribers', 'yt_subscribers_numeric'])\n",
    "    yt_view_col = first_existing_column(df_artists, ['yt_total_views', 'yt_total_views_numeric'])\n",
    "\n",
    "    tik_views_col = first_existing_column(df_artists, ['tiktok_views_numeric', 'tiktok_views'])\n",
    "    tik_vids_col = first_existing_column(df_artists, ['tiktok_video_count_numeric', 'tiktok_video_count'])\n",
    "\n",
    "    df_artists['yt_subscribers'] = pd.to_numeric(df_artists[yt_sub_col], errors='coerce').fillna(0)\n",
    "    df_artists['yt_total_views'] = pd.to_numeric(df_artists[yt_view_col], errors='coerce').fillna(0)\n",
    "\n",
    "    df_artists['tiktok_views'] = pd.to_numeric(df_artists[tik_views_col], errors='coerce').fillna(0)\n",
    "    df_artists['tiktok_video_count'] = pd.to_numeric(df_artists[tik_vids_col], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "    # ---------- NORMALIZATION (manual) ----------\n",
    "    df_artists['norm_yt_engagement'] = manual_minmax(df_artists['avg_yt_engagement'])\n",
    "    df_artists['norm_yt_views'] = manual_minmax(df_artists['yt_total_views'])\n",
    "    df_artists['norm_yt_subs'] = manual_minmax(df_artists['yt_subscribers'])\n",
    "\n",
    "    df_artists['norm_tiktok_views'] = manual_minmax(df_artists['tiktok_views'])\n",
    "    df_artists['norm_tiktok_posts'] = manual_minmax(df_artists['tiktok_video_count'])\n",
    "\n",
    "\n",
    "    # ---------- COMBINED POPULARITY INDEX ----------\n",
    "    df_artists['combined_popularity_index'] = (\n",
    "        df_artists['norm_yt_engagement'] * 0.35 +\n",
    "        df_artists['norm_yt_views'] * 0.20 +\n",
    "        df_artists['norm_yt_subs'] * 0.15 +\n",
    "        df_artists['norm_tiktok_views'] * 0.20 +\n",
    "        df_artists['norm_tiktok_posts'] * 0.10\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---------- SAVE RESULTS ----------\n",
    "    output_path = os.path.join(PROCESSED_DATA_DIR, \"analyzed_artists_data.csv\")\n",
    "    df_artists.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Saved analysis ‚Üí {output_path}\")\n",
    "\n",
    "    print(\"\\nüî• Top Artists by Popularity:\")\n",
    "    print(df_artists[['artist_name', 'combined_popularity_index']].sort_values(\n",
    "        'combined_popularity_index', ascending=False\n",
    "    ).head(10))\n",
    "\n",
    "    return df_artists\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Run script\n",
    "# ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df_artists, df_content = load_cleaned_data()\n",
    "    if df_artists.empty or df_content.empty:\n",
    "        print(\"‚ùå No data available.\")\n",
    "    else:\n",
    "        perform_analysis(df_artists, df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07191cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations...\n",
      "Visualizations complete.\n"
     ]
    }
   ],
   "source": [
    "# src/visualize_results.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Handle paths (works in Jupyter and VS Code)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, '../data/processed')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, '../results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "ANALYZED_ARTISTS_FP = os.path.join(PROCESSED_DATA_DIR, 'analyzed_artists_data.csv')\n",
    "ANALYZED_YT_AGE_FP = os.path.join(PROCESSED_DATA_DIR, 'analyzed_yt_age_performance.csv')\n",
    "\n",
    "\n",
    "def load_analyzed_data():\n",
    "    df_artists = pd.read_csv(ANALYZED_ARTISTS_FP) if os.path.exists(ANALYZED_ARTISTS_FP) else pd.DataFrame()\n",
    "    df_yt_age = pd.read_csv(ANALYZED_YT_AGE_FP) if os.path.exists(ANALYZED_YT_AGE_FP) else pd.DataFrame()\n",
    "    return df_artists, df_yt_age\n",
    "\n",
    "\n",
    "def save_fig(name):\n",
    "    path = os.path.join(RESULTS_DIR, name)\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    print(\"Saved:\", name)\n",
    "\n",
    "\n",
    "def create_visualizations(df_artists, df_yt_age):\n",
    "    print(\"Generating visualizations...\")\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1) Combined Popularity Index Bar Chart\n",
    "    # ---------------------------------------------------------\n",
    "    if not df_artists.empty and \"combined_popularity_index\" in df_artists.columns:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        df_sorted = df_artists.sort_values(\"combined_popularity_index\", ascending=False)\n",
    "        sns.barplot(\n",
    "            data=df_sorted,\n",
    "            x=\"artist_name\",\n",
    "            y=\"combined_popularity_index\",\n",
    "            palette=\"viridis\"\n",
    "        )\n",
    "        plt.title(\"Combined Popularity Index by Artist\", fontsize=16)\n",
    "        plt.xlabel(\"Artist\")\n",
    "        plt.ylabel(\"Combined Index\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        save_fig(\"combined_popularity_index.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2) YouTube Avg Views by Age Category\n",
    "    # ---------------------------------------------------------\n",
    "    if not df_yt_age.empty:\n",
    "        # ensure category ordering\n",
    "        age_order = [\"0‚Äì1 Month\", \"1‚Äì3 Months\", \"3‚Äì12 Months\", \"1‚Äì3 Years\", \"3+ Years\"]\n",
    "        if \"age_category\" in df_yt_age.columns:\n",
    "            df_yt_age[\"age_category\"] = pd.Categorical(df_yt_age[\"age_category\"], age_order, ordered=True)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.barplot(\n",
    "            data=df_yt_age,\n",
    "            x=\"age_category\",\n",
    "            y=\"avg_views\",\n",
    "            hue=\"artist_name\",\n",
    "            palette=\"deep\"\n",
    "        )\n",
    "        plt.title(\"Average YouTube Views by Age Category\", fontsize=16)\n",
    "        plt.xlabel(\"Age Category\")\n",
    "        plt.ylabel(\"Avg Views\")\n",
    "        plt.xticks(rotation=35)\n",
    "        plt.legend(title=\"Artist\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        save_fig(\"yt_avg_views_by_age.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) Release Frequency vs Popularity\n",
    "    # ---------------------------------------------------------\n",
    "    if (\n",
    "        not df_artists.empty\n",
    "        and \"combined_popularity_index\" in df_artists.columns\n",
    "        and \"avg_yt_videos_per_month\" in df_artists.columns\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.scatterplot(\n",
    "            data=df_artists,\n",
    "            x=\"avg_yt_videos_per_month\",\n",
    "            y=\"combined_popularity_index\",\n",
    "            hue=\"artist_name\",\n",
    "            s=160,\n",
    "            palette=\"tab10\"\n",
    "        )\n",
    "        plt.title(\"Upload Frequency vs Popularity\", fontsize=16)\n",
    "        plt.xlabel(\"Avg YouTube Videos per Month\")\n",
    "        plt.ylabel(\"Popularity Index\")\n",
    "        plt.tight_layout()\n",
    "        save_fig(\"release_freq_vs_popularity.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4) TikTok Views vs YouTube Subscribers\n",
    "    # ---------------------------------------------------------\n",
    "    required_cols = [\"tiktok_views\", \"subscriber_count\", \"artist_name\"]\n",
    "    if all(col in df_artists.columns for col in required_cols):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.scatterplot(\n",
    "            data=df_artists,\n",
    "            x=\"tiktok_views\",\n",
    "            y=\"subscriber_count\",\n",
    "            hue=\"artist_name\",\n",
    "            s=160,\n",
    "            palette=\"tab10\"\n",
    "        )\n",
    "        plt.title(\"TikTok Views vs YouTube Subscribers\", fontsize=16)\n",
    "        plt.xlabel(\"TikTok Views\")\n",
    "        plt.ylabel(\"YouTube Subscribers\")\n",
    "        plt.tight_layout()\n",
    "        save_fig(\"tiktok_vs_yt_subscribers.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5) Sentiment Breakdown (Donut Chart) for Top Artist\n",
    "    # ---------------------------------------------------------\n",
    "    if (\n",
    "        not df_artists.empty\n",
    "        and \"avg_sentiment_positive\" in df_artists.columns\n",
    "    ):\n",
    "        top = df_artists.sort_values(\"combined_popularity_index\", ascending=False).iloc[0]\n",
    "\n",
    "        pos = float(top.get(\"avg_sentiment_positive\", 0))\n",
    "        neg = float(top.get(\"avg_sentiment_negative\", 0)) if \"avg_sentiment_negative\" in df_artists.columns else 0\n",
    "        neu = float(top.get(\"avg_sentiment_neutral\", 0)) if \"avg_sentiment_neutral\" in df_artists.columns else 1 - pos - neg\n",
    "\n",
    "        vals = [max(pos, 0), max(neg, 0), max(neu, 0)]\n",
    "        labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.pie(vals, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "        centre = plt.Circle((0, 0), 0.65, color=\"white\")\n",
    "        plt.gca().add_artist(centre)\n",
    "        plt.title(f\"YouTube Sentiment for {top['artist_name']}\")\n",
    "        plt.tight_layout()\n",
    "        save_fig(f\"sentiment_{top['artist_name'].lower().replace(' ', '_')}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Visualizations complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_artists, df_yt_age = load_analyzed_data()\n",
    "    create_visualizations(df_artists, df_yt_age)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
